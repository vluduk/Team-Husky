{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f4411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Accelerated Forecasting: Baselines & MLPs\n",
    "# Dataset: 6-variable series, sampled once per minute\n",
    "# Task: Predict all 6 variables 4 hours (240 minutes) ahead\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ead8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ========= Device =========\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233e027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# ========= Data Loading =========\n",
    "df = pd.read_csv('Train_timeseries_filled.csv')\n",
    "data = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "# ========= Move to device and crop =========\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)[:100_000]\n",
    "\n",
    "# ========= Build one‐step forecast dataset =========\n",
    "# X[t] → predict X[t+1]\n",
    "X = data[:-6]   # (N-1, 6)\n",
    "Y = data[-6:]    # (N-1, 6)\n",
    "\n",
    "# ========= Train/test split (80/20) =========\n",
    "M = X.size(0)\n",
    "train_M = int(0.8 * M)\n",
    "X_train, Y_train = X[:train_M], Y[:train_M]\n",
    "X_test,  Y_test  = X[train_M:], Y[train_M:]\n",
    "\n",
    "# ========= Train Normalization =========\n",
    "train_mean = X_train.mean(dim=0, keepdim=True)  # (1,6)\n",
    "train_std  = X_train.std(dim=0, keepdim=True)   # (1,6)\n",
    "train_std[train_std == 0] = 1.0\n",
    "\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_test  = (X_test  - train_mean) / train_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8346e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.size(1) # 6 variables * hist_len\n",
    "num_targets = Y.size(1)   # 6 variables * horiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716cf6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43740c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Metrics =========\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    yt = y_true.cpu().numpy()\n",
    "    yp = y_pred.detach().cpu().numpy()\n",
    "    return {\n",
    "        'MSE': mean_squared_error(yt, yp),\n",
    "        'MAE': mean_absolute_error(yt, yp),\n",
    "        'R2' : r2_score(yt, yp)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b9403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 1. Naïve Forecast =========\n",
    "def predict_naive(X_batch):\n",
    "    # last observed 6 values → repeat for horizon\n",
    "    last = X_batch[:, -num_features:]                # (B,6)\n",
    "    return last.repeat(1, num_targets).view(-1, num_targets)  # (B,6*4)\n",
    "\n",
    "Y_pred_naive = predict_naive(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33decd54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 119994]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_pred_naive\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      3\u001b[0m yt \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      4\u001b[0m yp \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myp\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_absolute_error(yt, yp),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m : r2_score(yt, yp)\n\u001b[0;32m      9\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Vluduk\\Documents\\GitHub\\Team-Husky\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Vluduk\\Documents\\GitHub\\Team-Husky\\.venv\\lib\\site-packages\\sklearn\\metrics\\_regression.py:565\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;124;03m0.825...\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    563\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[0;32m    564\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 565\u001b[0m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m )\n\u001b[0;32m    569\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    570\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m _average((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Vluduk\\Documents\\GitHub\\Team-Husky\\.venv\\lib\\site-packages\\sklearn\\metrics\\_regression.py:198\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mregression task.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 198\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Vluduk\\Documents\\GitHub\\Team-Husky\\.venv\\lib\\site-packages\\sklearn\\metrics\\_regression.py:104\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 104\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    106\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Vluduk\\Documents\\GitHub\\Team-Husky\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 119994]"
     ]
    }
   ],
   "source": [
    "compute_metrics(Y_test, Y_pred_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 2. Moving Average =========\n",
    "def predict_moving_avg(X_batch, window=240):\n",
    "    # reshape to (B, hist_len, 6)\n",
    "    X_batch = X_batch.view(-1, 240, num_features)  # (B,240,6)\n",
    "    return X_batch.mean(dim=1).repeat(1, num_targets).view(-1, num_targets)  # (B,6*4)\n",
    "\n",
    "Y_pred_ma = predict_moving_avg(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1a33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 1.0803263187408447,\n",
       " 'MAE': 0.6927345991134644,\n",
       " 'R2': 0.43251562118530273}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(Y_test, Y_pred_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a86c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(X_train, Y_train, X_query, k=10, batch_size=1):\n",
    "    preds = []\n",
    "    for i in range(0, X_query.size(0), batch_size):\n",
    "        batch = X_query[i:i+batch_size]\n",
    "        dists = torch.cdist(batch, X_train)  # (Bq, Bt)\n",
    "        idx = torch.topk(dists, k, largest=False).indices\n",
    "        neigh = Y_train[idx]  # (Bq, k, D)\n",
    "        preds.append(neigh.mean(dim=1))\n",
    "    return torch.cat(preds, dim=0)\n",
    "\n",
    "Y_pred_knn = predict_knn(X_train, Y_train, X_test, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478baf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 1.9872112274169922,\n",
       " 'MAE': 0.9827815294265747,\n",
       " 'R2': -0.056001272052526474}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(Y_test, Y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78f2b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ========= 5. MLP (PyTorch) =========\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMLP\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_dim, hidden\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m], out_dim\u001b[38;5;241m=\u001b[39mnum_features):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# ========= 5. MLP (PyTorch) =========\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=[128, 64], out_dim=num_features):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [in_dim] + hidden\n",
    "        for i in range(len(hidden)):\n",
    "            layers += [nn.Linear(dims[i], dims[i+1]), nn.ReLU()]\n",
    "        layers.append(nn.Linear(dims[-1], out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(in_dim=num_features).to(device)\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ad699",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    mlp.train()\n",
    "    opt.zero_grad()\n",
    "    pred = mlp(X_train)\n",
    "    loss = loss_fn(pred, Y_train)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "Y_pred_mlp = mlp(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2463dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 1.1297732591629028,\n",
       " 'MAE': 0.7185354232788086,\n",
       " 'R2': 0.4065788984298706}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(Y_test, Y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c17f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive    | MSE: 1.0510 | MAE: 0.6816 | R2: 0.4477\n",
      "MovAvg   | MSE: 1.0803 | MAE: 0.6927 | R2: 0.4325\n",
      "KNN      | MSE: 1.9872 | MAE: 0.9828 | R2: -0.0560\n",
      "MLP      | MSE: 1.1298 | MAE: 0.7185 | R2: 0.4066\n",
      "FFT-MLP  | MSE: 2.1759 | MAE: 1.1708 | R2: -0.1492\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'Naive' : compute_metrics(Y_test, Y_pred_naive),\n",
    "    'MovAvg': compute_metrics(Y_test, Y_pred_ma),\n",
    "    'KNN'   : compute_metrics(Y_test, Y_pred_knn),\n",
    "    'MLP'   : compute_metrics(Y_test, Y_pred_mlp),\n",
    "}\n",
    "\n",
    "for name, mets in results.items():\n",
    "    print(f\"{name:8s} | MSE: {mets['MSE']:.4f} | MAE: {mets['MAE']:.4f} | R2: {mets['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4c04a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
