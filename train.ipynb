{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18EEWisFM_Qp",
        "outputId": "83a14bb1-55eb-4dc2-bd40-fc5d0cb91b17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33;20m[skrl:WARNING] Using `from skrl.envs.torch import ...` is deprecated and will be removed in future versions.\u001b[0m\n",
            "\u001b[33;20m[skrl:WARNING]  - Import loaders using `from skrl.envs.loaders.torch import ...`\u001b[0m\n",
            "\u001b[33;20m[skrl:WARNING]  - Import wrappers using `from skrl.envs.wrappers.torch import ...`\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gym  # use Gym for compatibility with SKRL's wrappers\n",
        "from gym.vector import SyncVectorEnv\n",
        "from gym.spaces import Box\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# SKRL imports\n",
        "from skrl.envs.torch import wrap_env\n",
        "from skrl.agents.torch.sac import SAC_DEFAULT_CONFIG, SAC\n",
        "from skrl.memories.torch import RandomMemory\n",
        "from skrl.trainers.torch import SequentialTrainer\n",
        "from skrl.models.torch import Model, GaussianMixin, DeterministicMixin\n",
        "from skrl.resources.preprocessors.torch import RunningStandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import tqdm\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9rHxsagNB_J"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesPredictionEnv(gym.Env):\n",
        "    \"\"\"Last 6 columns are targets, rest are features.\"\"\"\n",
        "    def __init__(self, data: pd.DataFrame, train_ratio: float = 0.8):\n",
        "        super().__init__()\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.scaler = StandardScaler()\n",
        "        # Feature/target split\n",
        "        self.n_targets = 6\n",
        "        self.feature_cols = data.columns[:-self.n_targets].tolist()\n",
        "        self.target_cols = data.columns[-self.n_targets:].tolist()\n",
        "        # Train/test split\n",
        "        train_size = int(len(data) * train_ratio)\n",
        "        train_df = data.iloc[:train_size]\n",
        "        test_df = data.iloc[train_size:]\n",
        "        # Fit & transform\n",
        "        X_train, X_test = train_df[self.feature_cols], test_df[self.feature_cols]\n",
        "        self.scaler.fit(X_train)\n",
        "        self.scaled_train_X = self.scaler.transform(X_train)\n",
        "        self.scaled_test_X  = self.scaler.transform(X_test)\n",
        "        self.train_y, self.test_y = train_df[self.target_cols].values, test_df[self.target_cols].values\n",
        "        # Spaces\n",
        "        obs_dim = len(self.feature_cols)\n",
        "        self.observation_space = Box(low=-20.0, high=20.0, shape=(obs_dim,), dtype=np.float32)\n",
        "        self.action_space      = Box(low=-20.0, high=20.0, shape=(self.n_targets,), dtype=np.float32)\n",
        "        # RNG and initial mode\n",
        "        self._np_random = None\n",
        "        self.set_mode(is_training=True)\n",
        "\n",
        "    @property\n",
        "    def np_random(self):\n",
        "        if self._np_random is None:\n",
        "            self._np_random = np.random.RandomState()\n",
        "        return self._np_random\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self._np_random = np.random.RandomState(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def set_mode(self, is_training=True):\n",
        "        self.is_training = is_training\n",
        "        self.current_x = self.scaled_train_X if is_training else self.scaled_test_X\n",
        "        self.current_y = self.train_y       if is_training else self.test_y\n",
        "        self.max_steps = len(self.current_x) - 1\n",
        "        self.current_step = (self.np_random.randint(0, self.max_steps)\n",
        "                             if is_training else 0)\n",
        "        return self.current_x[self.current_step]\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        self.current_step = (self.np_random.randint(0, self.max_steps)\n",
        "                             if self.is_training else 0)\n",
        "        obs = self.current_x[self.current_step]\n",
        "        return obs, {\"target\": self.current_y[self.current_step].copy()}\n",
        "\n",
        "    def step(self, action):\n",
        "        pred, actual = action, self.current_y[self.current_step]\n",
        "        print(f\"Pred: {pred}, Actual: {actual}\")\n",
        "        reward = -np.mean((pred - actual)**2)\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "        if done:\n",
        "            self.current_step = (self.np_random.randint(0, self.max_steps)\n",
        "                                 if self.is_training else 0)\n",
        "        obs = self.current_x[self.current_step]\n",
        "        return obs, reward, done, False, {\"target\": actual.copy()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_env(data, train_ratio, seed=None):\n",
        "    def _thunk():\n",
        "        env = TimeSeriesPredictionEnv(data, train_ratio)\n",
        "        if seed is not None:\n",
        "            env.seed(seed)\n",
        "        return env\n",
        "    return _thunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gWEx5X7ZVICd"
      },
      "outputs": [],
      "source": [
        "# Define the critic network (Q-function)\n",
        "class Critic(DeterministicMixin, Model):\n",
        "    def __init__(self, observation_space, action_space, device, clip_actions=False):\n",
        "        Model.__init__(self, observation_space, action_space, device)\n",
        "        DeterministicMixin.__init__(self, clip_actions)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.num_observations + self.num_actions, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def compute(self, inputs, role):\n",
        "        return self.net(torch.cat([inputs[\"states\"], inputs[\"taken_actions\"]], dim=1)), {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M91fXNgOUbE4"
      },
      "outputs": [],
      "source": [
        "# Define the actor network (policy)\n",
        "class Actor(GaussianMixin, Model):\n",
        "    def __init__(self, observation_space, action_space, device, clip_actions=False,\n",
        "                 clip_log_std=True, min_log_std=-20, max_log_std=2):\n",
        "        Model.__init__(self, observation_space, action_space, device)\n",
        "        GaussianMixin.__init__(self, clip_actions, clip_log_std, min_log_std, max_log_std)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.num_observations, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, self.num_actions)\n",
        "        )\n",
        "        self.log_std_parameter = nn.Parameter(torch.zeros(self.num_actions))\n",
        "\n",
        "    def compute(self, inputs, role):\n",
        "        return self.net(inputs[\"states\"]), self.log_std_parameter, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t4MjkjIMNI5E"
      },
      "outputs": [],
      "source": [
        "def train_sac(env, timesteps: int = 100000) -> SAC:\n",
        "    \"\"\"Train a Soft Actor-Critic agent across multiple envs using SKRL.\"\"\"\n",
        "    env = wrap_env(env)\n",
        "    # Instantiate models\n",
        "    models = {\n",
        "        \"policy\": Actor(env.observation_space, env.action_space, device),\n",
        "        \"critic_1\": Critic(env.observation_space, env.action_space, device),\n",
        "        \"critic_2\": Critic(env.observation_space, env.action_space, device),\n",
        "        \"target_critic_1\": Critic(env.observation_space, env.action_space, device),\n",
        "        \"target_critic_2\": Critic(env.observation_space, env.action_space, device)\n",
        "    }\n",
        "    for model in models.values():\n",
        "        model.init_parameters(method_name=\"normal_\", mean=0.0, std=0.1)\n",
        "    # SAC configuration\n",
        "    cfg = SAC_DEFAULT_CONFIG.copy()\n",
        "    cfg.update({\n",
        "        \"gradient_steps\": 1,\n",
        "        \"batch_size\": 256,\n",
        "        \"random_timesteps\": 1000,\n",
        "        \"learning_starts\": 1000,\n",
        "        \"learn_entropy\": True,\n",
        "        \"entropy_learning_rate\": 3e-4,\n",
        "        \"actor_learning_rate\": 3e-4,\n",
        "        \"critic_learning_rate\": 3e-4,\n",
        "        \"state_preprocessor\": RunningStandardScaler,\n",
        "        \"state_preprocessor_kwargs\": {\"size\": env.observation_space, \"device\": device},\n",
        "        \"discount_factor\": 0.99,\n",
        "        \"polyak\": 0.005,\n",
        "        \"experiment\": {\"write_interval\": 1000, \"checkpoint_interval\": 5000}\n",
        "    })\n",
        "    agent = SAC(\n",
        "        models=models,\n",
        "        memory=RandomMemory(memory_size=90000, num_envs=env.num_envs, device=device),\n",
        "        cfg=cfg,\n",
        "        observation_space=env.observation_space,\n",
        "        action_space=env.action_space,\n",
        "        device=device\n",
        "    )\n",
        "    # Trainer\n",
        "    cfg_trainer = {\"timesteps\": timesteps, \"headless\": True}\n",
        "    trainer = SequentialTrainer(cfg=cfg_trainer, env=env, agents=agent)\n",
        "    trainer.train()\n",
        "    \n",
        "    return agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading large dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[38;20m[skrl:INFO] Environment wrapper: 'auto' (class: gym.vector.vector_env.VectorEnv)\u001b[0m\n",
            "\u001b[38;20m[skrl:INFO] Environment wrapper: Gym\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training SAC agent with skrl...\n",
            "100%|██████████| 1000/1000 [00:02<00:00, 380.60it/s]\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "\n",
        "    # Load data (you can use nrows for testing with smaller dataset)\n",
        "    data = pd.read_csv(\"X.csv\")\n",
        "    data = data[:400_000]\n",
        "    \n",
        "\n",
        "    print(\"Loading large dataset...\")\n",
        "    num_envs = 128\n",
        "    train_ratio = 0.8\n",
        "    # `data` is your pandas DataFrame\n",
        "    env_fns = [make_env(data, train_ratio, seed=i) for i in range(num_envs)]\n",
        "    env = SyncVectorEnv(env_fns)\n",
        "\n",
        "\n",
        "    # Train agent\n",
        "    print(\"Training SAC agent with skrl...\")\n",
        "    agent = train_sac(env, timesteps=1000_000)\n",
        "\n",
        "    # Save the trained agent\n",
        "    agent.save(\"sac_timeseries_agent_large.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: raw string with backslashes\n",
        "agent.load(r\"runs\\25-04-27_03-32-27-656454_SAC\\checkpoints\\best_agent.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LNgOndufNK5c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def test_sac(env, agent: SAC, episodes: int = 2) -> None:\n",
        "    \"\"\"Evaluate a trained SAC agent, computing average MSE, MAE, and R2.\"\"\"\n",
        "    if hasattr(env, 'envs'):\n",
        "        for sub_env in env.envs:\n",
        "            if hasattr(sub_env, 'set_mode'):\n",
        "                sub_env.set_mode(is_training=False)\n",
        "\n",
        "    env_wrapped = wrap_env(env)\n",
        "\n",
        "    all_preds_rewards = []\n",
        "    max_steps = 0\n",
        "    if hasattr(env, 'envs') and len(env.envs) > 0:\n",
        "        max_steps = env.envs[0].max_steps\n",
        "    total_timesteps = episodes * env_wrapped.num_envs * max_steps\n",
        "    step_counter = 0\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env_wrapped.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                action = agent.act(obs, timestep=step_counter, timesteps=total_timesteps)[0]\n",
        "            obs, reward, done, truncated, info = env_wrapped.step(action)\n",
        "\n",
        "            # Collect predictions and targets\n",
        "            \n",
        "            done = done[0]\n",
        "\n",
        "            step_counter += 1\n",
        "            all_preds_rewards.append(reward)\n",
        "\n",
        "    all_preds_rewards = [\n",
        "        r.cpu().numpy() if torch.is_tensor(r) else r for r in all_preds_rewards\n",
        "    ]\n",
        "    all_preds_rewards = np.array(all_preds_rewards)\n",
        "    all_preds_rewards = all_preds_rewards.reshape(episodes, env_wrapped.num_envs, max_steps, -1)\n",
        "    all_preds_rewards = all_preds_rewards[:, :, :, 0]  # Select the first target only\n",
        "    all_preds_rewards = all_preds_rewards.reshape(-1, all_preds_rewards.shape[-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[38;20m[skrl:INFO] Environment wrapper: 'auto' (class: gym.vector.vector_env.VectorEnv)\u001b[0m\n",
            "\u001b[38;20m[skrl:INFO] Environment wrapper: Gym\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating the agent on test data...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEvaluating the agent on test data...\")\n",
        "test_sac(env, agent, episodes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B9e4QttWFDS"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(agent, env, data, series_idx=0):\n",
        "    \"\"\"Visualize predictions for a specific series\"\"\"\n",
        "    env = wrap_env(env)\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        action = agent.act(obs, timestep=0, timesteps=0)[0]\n",
        "\n",
        "    # Convert action to numpy and reshape\n",
        "    action_np = action.cpu().numpy()\n",
        "    predicted = action_np.reshape(env.unwrapped.prediction_horizon, env.unwrapped.n_series)\n",
        "\n",
        "    # Inverse transform predictions\n",
        "    predicted_original = env.unwrapped.scaler.inverse_transform(predicted)\n",
        "\n",
        "    # Get actual values\n",
        "    actual_idx_start = env.unwrapped.current_step + env.unwrapped.lookback_window\n",
        "    actual_idx_end = actual_idx_start + env.unwrapped.prediction_horizon\n",
        "    actual = data.iloc[actual_idx_start:actual_idx_end][env.unwrapped.series_cols].values\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(range(env.unwrapped.prediction_horizon), actual[:, series_idx],\n",
        "             label='Actual', marker='o')\n",
        "    plt.plot(range(env.unwrapped.prediction_horizon), predicted_original[:, series_idx],\n",
        "             label='Predicted', marker='x')\n",
        "    plt.title(f'Series {series_idx + 1} - 4-hour Ahead Prediction')\n",
        "    plt.xlabel('Minutes ahead')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsyV3F8_WIL8"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 3 (2354504556.py, line 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[185], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    visualize_predictions(agent, env, data, series_idx=i)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 3\n"
          ]
        }
      ],
      "source": [
        "    # Visualize some predictions\n",
        "print(\"\\nVisualizing predictions...\")\n",
        "for i in range(min(3, env.n_series)):  # Visualize up to 3 series\n",
        "    visualize_predictions(agent, env, data, series_idx=i)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
